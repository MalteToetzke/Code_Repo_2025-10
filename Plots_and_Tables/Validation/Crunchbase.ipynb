{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\"\"\"This Script compares the data from the Crunchbase dataset with the data from LinkedIn. Crunchbase data needs to be acquired directly from Crunchbase.\"\"\"",
   "id": "478ab153a80d4db6"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Crunchbase\n",
    "import pandas as pd\n",
    "\n",
    "# import the Crunchbase data; the data has to be acquired from Crunchbase directly\n",
    "directory = 'Path to Crunchbase Data'\n",
    "df_crunch = pd.read_csv(directory)\n",
    "\n",
    "# Import the LinkedIn data\n",
    "df_orgs = pd.read_csv(\"../Data/LinkedIn_Orgs_with_Websites.csv\", index=False)"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Preprocess Crunchbase data\n",
    "# Drop NaNs in the category column to avoid errors\n",
    "category_series = df_crunch['category_list'].dropna()\n",
    "\n",
    "# Split each row on commas, flatten the result, and remove whitespace\n",
    "all_categories = category_series.str.split(',').explode().str.strip()\n",
    "\n",
    "\n",
    "# Get unique categories\n",
    "unique_categories = all_categories.dropna().unique()\n",
    "\n",
    "# Convert to a sorted list if needed\n",
    "unique_categories_sorted = sorted(unique_categories)\n",
    "\n",
    "# Print or return the list\n",
    "print(unique_categories_sorted)\n",
    "\n",
    "#relevant categories from Crunchbase for Matching\n",
    "categories = ['CleanTech','Clean Energy', 'Carbon Capture','Battery','Biofuel','Biomass', 'Electric Vehicle','Fuel Cell','Geothermal Energy','Renewable Energy','Solar','Wind Energy']\n",
    "\n",
    "df_crunch_relevant = df_crunch[df_crunch['category_list'].str.contains('|'.join(categories), na=False)]\n",
    "# companies need to have received any funding\n",
    "df_crunch_relevant = df_crunch_relevant.dropna(subset=['total_funding'])\n",
    "print(df_crunch_relevant.shape[0]) # yields 7937 companies\n",
    "\n",
    "# make last_funding_on a date and then only take funding after 2020\n",
    "df_crunch_relevant['last_funding_on'] = pd.to_datetime(df_crunch_relevant['last_funding_on'], errors='coerce')\n",
    "# take funding aft3r 2020\n",
    "df_crunch_relevant['last_funding_on'] = df_crunch_relevant['last_funding_on'].dt.year\n",
    "# take funding after 2020\n",
    "df_crunch_relevant['last_funding_on'] = df_crunch_relevant['last_funding_on'].fillna(0)\n",
    "df_crunch_relevant = df_crunch_relevant[df_crunch_relevant['last_funding_on'] > 2019]\n",
    "print(df_crunch_relevant.shape[0]) # yields 5090 companies\n",
    "\n",
    "# only take companies with LinkedIn Url\n",
    "df_crunch_relevant = df_crunch_relevant.dropna(subset=['linkedin_url']) # yields 4564 companies\n"
   ],
   "id": "2c9bc758dce5a726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge the Crunchbase data with the LinkedIn data\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_linkedin_key(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    match = re.search(r'linkedin\\.com/company/([^/?#]+)/?', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def extract_domain_stem_crunchbase(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    # Regex: get the first part before common TLDs\n",
    "    match = re.match(r'^([a-zA-Z0-9-]+)\\.(?:com|org|net|io|gov|edu|co|sg|com\\.sg|co\\.uk|co\\.nz|ca|de|fr|eu|us|uk)(?:\\..+)?$', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    # Fallback: just return the first part before the first dot\n",
    "    return url.split('.')[0]\n",
    "\n",
    "def extract_domain_body(url):\n",
    "    # Extract the domain name while ignoring subdomains like 'www', 'en', 'co', etc.\n",
    "    match = re.search(r'https?://(?:[a-zA-Z0-9-]+\\.)*?([a-zA-Z0-9-]+)\\.(?:[a-z]{2,})', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_domain_body_from_Linkedin(df, column):\n",
    "    vector = df[column].str.replace(\"www.\",\"\").str.replace(\"/en.\",\"/\").str.replace(\"/new.\",\"/\").apply(lambda x: extract_domain_body(x) if pd.notna(x) else None)\n",
    "    return vector\n",
    "\n",
    "def extract_crunchbase_id(url):\n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    match = re.search(r'crunchbase\\.com/organization/([^/?#]+)', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ],
   "id": "3908b7a20b70e590"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# extract the string after the last /\n",
    "df_crunch_relevant[\"Linkedin_id\"] = df_crunch_relevant['linkedin_url'].apply(extract_linkedin_key)\n",
    "#df_crunch_relevant = df_crunch_relevant[(df_crunch_relevant.Linkedin_id.str.contains(\"?\", regex=False)==False)]\n",
    "df_crunch_relevant = df_crunch_relevant[(df_crunch_relevant.status==\"operating\")|(df_crunch_relevant.status==\"ipo\")]\n",
    "\n",
    "# extract the domain name from the website\n",
    "df_crunch_relevant['domain'] = df_crunch_relevant['domain'].apply(lambda x: extract_domain_stem_crunchbase(x) if pd.notna(x) else None)\n",
    "\n",
    "# extract the crunchbase id from cb_url_x\n",
    "df_crunch_relevant[\"crunchbase_id\"] = df_crunch_relevant['cb_url'].apply(extract_crunchbase_id)\n",
    "\n",
    "# extract the Linkedin_id\n",
    "df_orgs[\"Linkedin_id\"] = df_orgs[\"linkedinUrl\"].astype(str)\n",
    "df_orgs[\"Linkedin_id\"] = df_orgs[\"Linkedin_id\"].apply(extract_linkedin_key)\n",
    "\n",
    "# extract the domain name from the website\n",
    "df_orgs['domain'] = extract_domain_body_from_Linkedin(df_orgs, 'website')\n",
    "\n",
    "# extract the crunchbase_id from crunchbase_url\n",
    "df_orgs[\"crunchbase_id\"] = df_orgs['crunchbaseUrl'].apply(extract_crunchbase_id)"
   ],
   "id": "e6b96e501164533f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#### Merging\n",
    "# Copy source DataFrames\n",
    "df_crunch = df_crunch_relevant.copy()\n",
    "df_org = df_orgs.copy()\n",
    "\n",
    "# Keep only needed columns from orgs\n",
    "org_cols = [\"Linkedin_id\", \"domain\", \"crunchbase_id\", \"name\", \"firm_type\", \"sector\"]\n",
    "df_org = df_org[org_cols].drop_duplicates()\n",
    "df_org = df_org.drop_duplicates(subset='Linkedin_id')\n",
    "\n",
    "# ---- Step 1: LinkedIn ID ----\n",
    "df1 = df_crunch.set_index(\"Linkedin_id\")\n",
    "print(df1.shape[0])\n",
    "df2 = df_org.set_index(\"Linkedin_id\")\n",
    "\n",
    "step1 = df1.join(df2, how=\"left\", rsuffix=\"_org\").reset_index()\n",
    "step1[\"matched\"] = 0 \n",
    "step1[\"matched\"][(step1[\"name_org\"].isnull())==False] = 1\n",
    "print(step1.shape[0])\n",
    "step1[\"match_type\"] = step1[\"matched\"].map({1: \"linkedin\", 0: None})\n",
    "print(step1.matched.mean())"
   ],
   "id": "142b5a67e13988db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---- Step 2: Domain ----\n",
    "unmatched1 = step1[step1[\"matched\"] == 0].copy()\n",
    "matched1 = step1[step1[\"matched\"] == 1].copy()\n",
    "\n",
    "\n",
    "print(unmatched1)\n",
    "# Drop columns to avoid duplication before merging. drop domain_org crunchbase_id_org\n",
    "unmatched1 = unmatched1.drop(columns=[\"name_org\", \"domain_org\", \"crunchbase_id_org\",\"firm_type\", \"sector\"])\n",
    "\n",
    "df1 = unmatched1.set_index(\"domain\")\n",
    "df2_domain = df_org.drop_duplicates(\"domain\")\n",
    "df2 = df2_domain.set_index(\"domain\")\n",
    "step2 = df1.join(df2, how=\"left\", rsuffix=\"_org\").reset_index()\n",
    "\n",
    "step2[\"matched\"] = 0\n",
    "step2[\"matched\"][(step2[\"name_org\"].isnull())==False] = 1\n",
    "step2[\"match_type\"] = step2[\"matched\"].map({1: \"domain\", 0: None})\n",
    "\n",
    "print(step2.matched.mean())"
   ],
   "id": "7e00b8d107da43be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---- Step 3: Crunchbase ID ----\n",
    "unmatched2 = step2[step2[\"matched\"] == 0].copy()\n",
    "matched2 = step2[step2[\"matched\"] == 1].copy()\n",
    "\n",
    "unmatched2 = unmatched2.drop(columns=[\"name_org\", \"crunchbase_id_org\",\"firm_type\", \"sector\"])\n",
    "\n",
    "\n",
    "df1 = unmatched2.set_index(\"crunchbase_id\")\n",
    "df2 = df_org.drop_duplicates(\"crunchbase_id\")\n",
    "df2 = df2.set_index(\"crunchbase_id\")\n",
    "\n",
    "step3 = df1.join(df2, how=\"left\", rsuffix=\"_org\").reset_index()\n",
    "\n",
    "step3[\"matched\"] = 0\n",
    "step3[\"matched\"][(step3[\"name_org\"].isnull())==False] = 1\n",
    "step3[\"match_type\"] = step3[\"matched\"].map({1: \"crunchbase\", 0: None})\n",
    "\n",
    "\n",
    "print(matched1.columns)#\n",
    "print(matched2.columns)\n",
    "print(step3.columns)"
   ],
   "id": "2ef44b45dc132f3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "def clean_duplicate_columns(df):\n",
    "    # Remove duplicated column names while preserving first occurrence\n",
    "    _, idx = np.unique(df.columns, return_index=True)\n",
    "    return df.iloc[:, np.sort(idx)]\n",
    "\n",
    "# Apply to all dataframes you're about to concatenate\n",
    "matched1 = clean_duplicate_columns(matched1)\n",
    "matched2 = clean_duplicate_columns(matched2)\n",
    "step3 = clean_duplicate_columns(step3)\n",
    "\n",
    "\n",
    "# ---- Combine all safely ----\n",
    "final_df = pd.concat([matched1, matched2, step3], ignore_index=True)\n",
    "\n",
    "print(\"âœ… Final shape:\", final_df.shape) # yields (4310,5)\n",
    "print(\"ðŸ”¢ Match rate:\", final_df[\"matched\"].mean()) # yields 0.6921"
   ],
   "id": "c311693cd763b419"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Country overview by country_code \n",
    "df_country = final_df\n",
    "\n",
    "# grooup by country_code create mean\n",
    "df_country = df_country[['country_code','matched','name']].groupby('country_code').agg({'matched':'mean','name':'count'}).reset_index()\n",
    "#sort descending by name\n",
    "df_country = df_country.sort_values(by='name', ascending=False)\n",
    "#rename name as number of organizations\n",
    "df_country = df_country.rename(columns={'name':'number_of_organizations'})\n",
    "print(df_country)\n",
    "\n",
    "# save as csv\n",
    "df_country.to_csv('country_overview_matched_startups_crunchbase.csv', index=False)"
   ],
   "id": "10174d93e33f3e39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# overview for employee_count\n",
    "df_employees = final_df\n",
    "# grooup by employee_count create mean\n",
    "df_employees = df_employees[['employee_count','matched','name']].groupby('employee_count').agg({'matched':'mean','name':'count'}).reset_index()\n",
    "#sort descending by name\n",
    "df_employees = df_employees.sort_values(by='name', ascending=False)\n",
    "#rename name as number of organizations\n",
    "df_employees = df_employees.rename(columns={'name':'number_of_organizations'})\n",
    "\n",
    "print(df_employees)\n",
    "# save as csv\n",
    "df_employees.to_csv('employee_count_overview_matched_startups_crunchbase.csv', index=False)"
   ],
   "id": "1fa6cf7457421d89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# overview by series raised\n",
    "\n",
    "# check by series: \n",
    "df_series =  pd.read_csv('path to funding_rounds data of Crunchbase')\n",
    "#drop the column name\n",
    "df_series = df_series.drop(columns=['name'])\n",
    "\n",
    "# drop duplicates of investment_type and name, keep the one with the latest \"announced_on\" date\n",
    "df_series = df_series.sort_values(by='announced_on', ascending=False)\n",
    "df_series = df_series.drop_duplicates(subset=['org_name'], keep='first')\n",
    "\n",
    "# only keep investments after 2020\n",
    "df_series['announced_on'] = pd.to_datetime(df_series['announced_on'], errors='coerce')\n",
    "df_series['announced_on'] = df_series['announced_on'].dt.year\n",
    "df_series['announced_on'] = df_series['announced_on'].fillna(0)\n",
    "df_series = df_series[df_series['announced_on'] > 2019]\n",
    "\n",
    "# merge with series left\n",
    "df_matched = final_df.merge(df_series, left_on='name', right_on='org_name', how='left')\n",
    "\n",
    "print(df_matched)\n",
    "df_matched.to_csv(\"matched_startups_crunchbase_by_investment.csv\", index=False)\n",
    "# group by investment_type mean and total\n",
    "df_matched = df_matched[['investment_type','matched','name']].groupby('investment_type').agg({'matched':'mean','name':'count'}).reset_index()\n",
    "#sort descending by name\n",
    "df_matched = df_matched.sort_values(by='name', ascending=False)\n",
    "\n",
    "#rename name as number of organizations\n",
    "df_matched = df_matched.rename(columns={'name':'number_of_organizations'})\n",
    "print(df_matched)\n",
    "# save as csv\n",
    "df_matched.to_csv('investment_type_overview_matched_startups_crunchbase.csv', index=False)"
   ],
   "id": "9b1b67c7a4d11233"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
